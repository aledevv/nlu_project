**TODO**

- replace RNN with LSTM
- add 2 dropout layers
    - 1 after embedding layer
    - 1 before the last linear layer
- replase SGD with AdamW

# Experiment also with a smaller or bigger model by changing hid and emb sizes 
# Don't forget to experiment with a lower training batch size
# Increasing the back propagation steps can be seen as a regularization step